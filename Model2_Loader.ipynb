{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fe7f07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from sklearn.utils import compute_class_weight\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f985c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6420, 100, 12)\n",
      "(6959,)\n"
     ]
    }
   ],
   "source": [
    "X_attacker = np.load(Path(\"ClosestCarCSVsModel2/np_attacker.npy\")) #ClosestCarCSVsModel2/np_attacker.npy\n",
    "X_normal = np.load(Path(\"ClosestCarCSVsModel2/np_normal.npy\"))\n",
    "\n",
    "X_normal_train, X_normal_test = train_test_split(X_normal, test_size=0.10, shuffle=True)\n",
    "X_attacker_train, X_attacker_test = train_test_split(X_attacker, test_size=0.33, shuffle=True)\n",
    "\n",
    "#If I want to even out the test set just chop some of the X_normal_test\n",
    "X_test = np.concatenate((X_normal_test, X_attacker_test), axis=0)\n",
    "y_test = np.asarray([0]*X_normal_test.shape[0] + [1]*X_attacker_test.shape[0]).astype('float32')\n",
    "print(X_attacker_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y_test) / y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(X_attacker, X_normal):\n",
    "    X_normal_split, p = train_test_split(X_normal, test_size=0.80, shuffle=True)\n",
    "    y = np.asarray([0]*X_normal_split.shape[0] + [1]*X_attacker.shape[0]).astype('float32')\n",
    "    X = np.concatenate((X_normal_split, X_attacker), axis=0)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_data(X_attacker_train, X_normal_train)\n",
    "np.sum(y_train)/len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_feats = 12\n",
    "# X_train = np.expand_dims(X_train, axis=-1)\n",
    "# X_test = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "N = 64\n",
    "model = Sequential()\n",
    "model.add(Conv2D(N, (3, 3), activation='relu', input_shape=(X_train.shape[1], n_feats, 1), padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(N*2, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(N*4, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(N*16, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# model = resnet50(input_shape=(100, 12, 1), num_classes=2)\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "mc = ModelCheckpoint('best_model2_V3.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "for i in range(0,400):\n",
    "    print('Epoch:', i)\n",
    "    X_train, y_train = get_data(X_attacker_train, X_normal_train)\n",
    "#     class_weights = compute_class_weight(class_weight = \"balanced\", classes= np.unique(y_train), y= y_train)\n",
    "#     class_weights = dict(zip(np.unique(y_train), class_weights))\n",
    "    \n",
    "    model.fit(X_train, y_train, \n",
    "              epochs=1, \n",
    "              batch_size=30000, \n",
    "#               class_weight=class_weights,\n",
    "#               shuffle=True,\n",
    "#               validation_split=0.1,\n",
    "#               callbacks=[mc],\n",
    "              verbose = 0\n",
    "              )\n",
    "\n",
    "model.save('model2_V3.h5')\n",
    "y_pred = model.predict(X_test, batch_size=9000)\n",
    "y_pred[y_pred > 0.5] = 1\n",
    "y_pred[y_pred <= 0.5] = 0\n",
    "y_pred.flatten()\n",
    "\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred.flatten()))\n",
    "print('Precision:', precision_score(y_test, y_pred.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Loading model from HDF5 will not restore the optimizer's weights, since the optimizer is an instance of `optimizer_experimental.Optimizer`\n",
      "1/1 [==============================] - 0s 62ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [6959, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m y_pred[y_pred \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      5\u001b[0m y_pred\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecision:\u001b[39m\u001b[38;5;124m'\u001b[39m, precision_score(y_test, y_pred\u001b[38;5;241m.\u001b[39mflatten()))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/_param_validation.py:192\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m validate_parameter_constraints(\n\u001b[1;32m    188\u001b[0m     parameter_constraints, params, caller_name\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\n\u001b[1;32m    189\u001b[0m )\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    200\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    202\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:221\u001b[0m, in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:86\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     88\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    400\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [6959, 1]"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('model2_V3.h5')\n",
    "y_pred = model.predict(X_test[0:1])\n",
    "y_pred[y_pred > 0.5] = 1\n",
    "y_pred[y_pred <= 0.5] = 0\n",
    "y_pred.flatten()\n",
    "\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred.flatten()))\n",
    "print('Precision:', precision_score(y_test, y_pred.flatten()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Accuracy: 0.7247211227060093\n",
    "Precision: 0.7807228915662651"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('model2_V2_allscale.h5')\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred.flatten()))\n",
    "print('Precision:', precision_score(y_test, y_pred.flatten()))\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred.flatten()).ravel()\n",
    "print(f'TN:{tn}, FP:{fp}, FN:{fn}, TP:{tp}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_feats = 16\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(64, input_shape=(None, n_feats)))) # \"None\" allows for varying input lengths\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "mc = ModelCheckpoint('best_model2_V2.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "for i in range(0,100):\n",
    "    print('Epoch:', i)\n",
    "    X_train, y_train = get_data(X_attacker_train, X_normal_train)\n",
    "    model.fit(X_train, y_train, \n",
    "              epochs=1, \n",
    "              batch_size=9000, \n",
    "              validation_split=0.1,\n",
    "              callbacks=[mc])\n",
    "\n",
    "model.save('model2_V2.h5')\n",
    "y_pred = model.predict(X_test, batch_size=2000)\n",
    "y_pred[y_pred > 0.5] = 1\n",
    "y_pred[y_pred <= 0.5] = 0\n",
    "y_pred.flatten()\n",
    "\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred.flatten()))\n",
    "print('Precision:', precision_score(y_test, y_pred.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred.flatten()).ravel()\n",
    "print(f'TN:{tn}, FP:{fp}, FN:{fn}, TP:{tp}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c84cc6",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "https://keras.io/examples/nlp/text_classification_with_transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4d6c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e21cdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalMaxPooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.layers import TransformerBlock\n",
    "\n",
    "# Prepare the data\n",
    "input_shape = (100,12)  # input shape of each time series sequence\n",
    "num_classes = 2  # number of classes for classification\n",
    "\n",
    "# Define the model\n",
    "inputs = Input(shape=input_shape)\n",
    "x = inputs\n",
    "embedding_dim = 12 \n",
    "num_heads = 8\n",
    "ff_dim = 12 \n",
    "\n",
    "# Time series embedding layer\n",
    "x = TransformerBlock(embed_dim=embedding_dim, num_heads=num_heads, ff_dim=ff_dim)(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "n_feats = 12\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "mc = ModelCheckpoint('best_model2_V2.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "# for i in range(0,100):\n",
    "print('Epoch:', i)\n",
    "X_train, y_train = get_data(X_attacker_train, X_normal_train)\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, \n",
    "          epochs=10, \n",
    "          batch_size=1000, \n",
    "          validation_split=0.1,\n",
    "          callbacks=[mc])\n",
    "\n",
    "model.save('model2_V2.h5')\n",
    "y_pred = model.predict(X_test, batch_size=2000)\n",
    "y_pred[y_pred > 0.5] = 1\n",
    "y_pred[y_pred <= 0.5] = 0\n",
    "y_pred.flatten()\n",
    "\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred.flatten()))\n",
    "print('Precision:', precision_score(y_test, y_pred.flatten()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
