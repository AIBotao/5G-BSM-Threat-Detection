{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fe7f07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Bidirectional\n",
    "# from keras.layers import LSTM, Dense, Bidirectional, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f985c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1196218, 10, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.load(Path('AllCSVs/X.npy'))\n",
    "y = np.load(Path('AllCSVs/y.npy'))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2e3e0d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.58773653e+03,  5.75352757e+03, -2.86077262e-01,\n",
       "         3.08101008e+01],\n",
       "       [ 3.58770667e+03,  5.78429871e+03, -2.98157224e-02,\n",
       "         3.07308358e+01],\n",
       "       [ 3.58767682e+03,  5.81506609e+03, -2.98966571e-02,\n",
       "         3.08142545e+01],\n",
       "       ...,\n",
       "       [ 6.11386363e+03,  5.82046746e+03, -9.41452437e+00,\n",
       "        -9.48456779e-01],\n",
       "       [ 6.10371394e+03,  5.81944494e+03, -1.07471562e+01,\n",
       "        -1.08271144e+00],\n",
       "       [ 3.59252004e+03,  5.71502826e+03,  1.46458078e+00,\n",
       "        -3.40739201e+01]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e823fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(801466, 10, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, shuffle=True)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dab1d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reshape input to be [samples, time steps, features]\n",
    "# X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "# X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "# X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03e7c45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "214/214 [==============================] - 7s 13ms/step - loss: 13414229.0000 - val_loss: 12496584.0000\n",
      "Epoch 2/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 10753140.0000 - val_loss: 8723078.0000\n",
      "Epoch 3/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 6639113.0000 - val_loss: 4599643.0000\n",
      "Epoch 4/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 3165828.7500 - val_loss: 1830604.5000\n",
      "Epoch 5/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 1317312.1250 - val_loss: 667707.0000\n",
      "Epoch 6/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 713592.5000 - val_loss: 366445.9688\n",
      "Epoch 7/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 600934.2500 - val_loss: 312761.6562\n",
      "Epoch 8/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 587373.1250 - val_loss: 303134.7500\n",
      "Epoch 9/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 584970.8750 - val_loss: 301439.8750\n",
      "Epoch 10/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 585294.5625 - val_loss: 301078.2188\n",
      "Epoch 11/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 583876.1875 - val_loss: 300772.4688\n",
      "Epoch 12/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 583130.4375 - val_loss: 300845.7812\n",
      "Epoch 13/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 581533.4375 - val_loss: 300863.3750\n",
      "Epoch 14/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 579823.6250 - val_loss: 300358.4375\n",
      "Epoch 15/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 578753.1875 - val_loss: 301142.9062\n",
      "Epoch 16/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 576200.8125 - val_loss: 301502.6250\n",
      "Epoch 17/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 575049.5625 - val_loss: 301525.9062\n",
      "Epoch 18/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 572340.6250 - val_loss: 301380.8438\n",
      "Epoch 19/3000\n",
      "214/214 [==============================] - 2s 11ms/step - loss: 571613.0625 - val_loss: 301399.0000\n",
      "Epoch 20/3000\n",
      "214/214 [==============================] - 2s 11ms/step - loss: 569665.0625 - val_loss: 301728.8750\n",
      "Epoch 21/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 567441.5625 - val_loss: 302706.4062\n",
      "Epoch 22/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 564905.2500 - val_loss: 303517.3125\n",
      "Epoch 23/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 563304.8750 - val_loss: 303257.4688\n",
      "Epoch 24/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 560715.8750 - val_loss: 304368.4688\n",
      "Epoch 25/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 560200.5625 - val_loss: 303785.1562\n",
      "Epoch 26/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 556594.4375 - val_loss: 304131.0938\n",
      "Epoch 27/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 553940.5625 - val_loss: 305010.3438\n",
      "Epoch 28/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 552321.5000 - val_loss: 305446.8125\n",
      "Epoch 29/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 549453.5000 - val_loss: 305357.5625\n",
      "Epoch 30/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 546746.1250 - val_loss: 305659.4375\n",
      "Epoch 31/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 542310.1250 - val_loss: 306211.8125\n",
      "Epoch 32/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 542210.1250 - val_loss: 307497.3750\n",
      "Epoch 33/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 537161.7500 - val_loss: 307879.9062\n",
      "Epoch 34/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 531452.9375 - val_loss: 306297.3750\n",
      "Epoch 35/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 529886.3750 - val_loss: 307556.4688\n",
      "Epoch 36/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 525156.3125 - val_loss: 308056.9375\n",
      "Epoch 37/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 522159.3125 - val_loss: 306213.3438\n",
      "Epoch 38/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 516932.9688 - val_loss: 308442.3125\n",
      "Epoch 39/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 513537.5000 - val_loss: 306178.5312\n",
      "Epoch 40/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 508044.5000 - val_loss: 307843.9688\n",
      "Epoch 41/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 503183.7188 - val_loss: 306194.6562\n",
      "Epoch 42/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 496962.1562 - val_loss: 307012.8125\n",
      "Epoch 43/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 492742.3750 - val_loss: 308623.0938\n",
      "Epoch 44/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 487145.8438 - val_loss: 306202.0000\n",
      "Epoch 45/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 481385.5938 - val_loss: 308507.1562\n",
      "Epoch 46/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 473665.0625 - val_loss: 309690.5938\n",
      "Epoch 47/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 468336.3438 - val_loss: 307208.4688\n",
      "Epoch 48/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 460571.8438 - val_loss: 307099.4375\n",
      "Epoch 49/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 453587.3750 - val_loss: 307113.4062\n",
      "Epoch 50/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 445921.8750 - val_loss: 306541.9688\n",
      "Epoch 51/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 437609.6875 - val_loss: 306505.5312\n",
      "Epoch 52/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 429130.2500 - val_loss: 307163.3125\n",
      "Epoch 53/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 420866.8125 - val_loss: 308243.5312\n",
      "Epoch 54/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 412906.7500 - val_loss: 306459.5625\n",
      "Epoch 55/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 403431.0625 - val_loss: 305133.3438\n",
      "Epoch 56/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 394911.6875 - val_loss: 306887.5938\n",
      "Epoch 57/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 386311.5625 - val_loss: 304541.5938\n",
      "Epoch 58/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 377787.1562 - val_loss: 304017.0938\n",
      "Epoch 59/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 369172.0625 - val_loss: 303980.5000\n",
      "Epoch 60/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 360446.1562 - val_loss: 305041.4062\n",
      "Epoch 61/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 352729.9062 - val_loss: 301698.0938\n",
      "Epoch 62/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 345536.0312 - val_loss: 302986.8750\n",
      "Epoch 63/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 338538.5312 - val_loss: 303672.7500\n",
      "Epoch 64/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 332119.2812 - val_loss: 299427.3438\n",
      "Epoch 65/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 326297.7188 - val_loss: 299425.4375\n",
      "Epoch 66/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 320904.5312 - val_loss: 298879.5625\n",
      "Epoch 67/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 316194.6875 - val_loss: 299180.0000\n",
      "Epoch 68/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 312061.6875 - val_loss: 298259.1250\n",
      "Epoch 69/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 308322.7812 - val_loss: 297801.7500\n",
      "Epoch 70/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 305637.4062 - val_loss: 298066.6562\n",
      "Epoch 71/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 303179.1875 - val_loss: 296274.3438\n",
      "Epoch 72/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 301044.3750 - val_loss: 296771.3750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 299470.2188 - val_loss: 295979.3125\n",
      "Epoch 74/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 298153.6875 - val_loss: 295236.3125\n",
      "Epoch 75/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 297186.1875 - val_loss: 295198.5625\n",
      "Epoch 76/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 296505.4688 - val_loss: 295422.8125\n",
      "Epoch 77/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 295943.6875 - val_loss: 295186.8125\n",
      "Epoch 78/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 295523.1875 - val_loss: 294975.7188\n",
      "Epoch 79/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 295345.6250 - val_loss: 294868.7500\n",
      "Epoch 80/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 295036.9062 - val_loss: 294999.0625\n",
      "Epoch 81/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294925.8125 - val_loss: 294637.2188\n",
      "Epoch 82/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294824.4688 - val_loss: 294552.9688\n",
      "Epoch 83/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294724.9062 - val_loss: 294868.4375\n",
      "Epoch 84/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294656.9062 - val_loss: 294646.4375\n",
      "Epoch 85/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294627.3750 - val_loss: 294566.8438\n",
      "Epoch 86/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294577.5000 - val_loss: 294555.3750\n",
      "Epoch 87/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294574.4688 - val_loss: 294535.1250\n",
      "Epoch 88/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294531.3750 - val_loss: 294564.1250\n",
      "Epoch 89/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294498.0312 - val_loss: 294570.2188\n",
      "Epoch 90/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294462.7812 - val_loss: 294558.0000\n",
      "Epoch 91/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294471.0625 - val_loss: 294540.0625\n",
      "Epoch 92/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294453.8125 - val_loss: 294604.2500\n",
      "Epoch 93/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294451.7812 - val_loss: 294547.3438\n",
      "Epoch 94/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294440.0938 - val_loss: 294669.8438\n",
      "Epoch 95/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294405.3750 - val_loss: 294562.8125\n",
      "Epoch 96/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294402.5625 - val_loss: 294547.7500\n",
      "Epoch 97/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294408.7812 - val_loss: 294589.4062\n",
      "Epoch 98/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294417.2812 - val_loss: 294536.1250\n",
      "Epoch 99/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294386.6250 - val_loss: 294544.8438\n",
      "Epoch 100/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294364.1250 - val_loss: 294537.5625\n",
      "Epoch 101/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294380.4688 - val_loss: 294652.3438\n",
      "Epoch 102/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294395.4375 - val_loss: 294544.1875\n",
      "Epoch 103/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294374.1875 - val_loss: 294548.5625\n",
      "Epoch 104/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294336.3750 - val_loss: 294539.0000\n",
      "Epoch 105/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294371.3750 - val_loss: 294581.5625\n",
      "Epoch 106/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294366.0938 - val_loss: 294580.0312\n",
      "Epoch 107/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294333.3125 - val_loss: 294568.2812\n",
      "Epoch 108/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294392.0000 - val_loss: 294578.2500\n",
      "Epoch 109/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294346.1875 - val_loss: 294591.6875\n",
      "Epoch 110/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294369.0625 - val_loss: 294548.4688\n",
      "Epoch 111/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294358.6250 - val_loss: 294540.5625\n",
      "Epoch 112/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294337.4375 - val_loss: 294536.3750\n",
      "Epoch 113/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294345.0938 - val_loss: 294640.5625\n",
      "Epoch 114/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294334.8438 - val_loss: 294558.0000\n",
      "Epoch 115/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294329.8125 - val_loss: 294542.6562\n",
      "Epoch 116/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294322.8750 - val_loss: 294537.9688\n",
      "Epoch 117/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294329.2812 - val_loss: 294551.5312\n",
      "Epoch 118/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294320.1250 - val_loss: 294567.4688\n",
      "Epoch 119/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294325.3750 - val_loss: 294543.4062\n",
      "Epoch 120/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294339.3438 - val_loss: 294540.9375\n",
      "Epoch 121/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294324.9062 - val_loss: 294573.8438\n",
      "Epoch 122/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 294335.4688 - val_loss: 294572.8438\n",
      "Epoch 123/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 272594.0938 - val_loss: 219043.8594\n",
      "Epoch 124/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 232742.7188 - val_loss: 197496.7031\n",
      "Epoch 125/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 224765.0469 - val_loss: 188044.5156\n",
      "Epoch 126/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 217511.4219 - val_loss: 183971.7188\n",
      "Epoch 127/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 210331.9375 - val_loss: 170763.2656\n",
      "Epoch 128/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 204724.4062 - val_loss: 161960.1094\n",
      "Epoch 129/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 198815.4688 - val_loss: 153989.2031\n",
      "Epoch 130/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 193693.2188 - val_loss: 147146.3281\n",
      "Epoch 131/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 189711.9375 - val_loss: 142179.0469\n",
      "Epoch 132/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 187076.0156 - val_loss: 136498.7812\n",
      "Epoch 133/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 185690.5938 - val_loss: 134124.0000\n",
      "Epoch 134/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 185103.1719 - val_loss: 131506.4062\n",
      "Epoch 135/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 183964.0469 - val_loss: 129706.9297\n",
      "Epoch 136/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 183742.3281 - val_loss: 129767.5703\n",
      "Epoch 137/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 183763.8594 - val_loss: 128143.3672\n",
      "Epoch 138/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 182937.5781 - val_loss: 127980.1094\n",
      "Epoch 139/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 183452.5469 - val_loss: 127762.0234\n",
      "Epoch 140/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 183194.6250 - val_loss: 127274.9844\n",
      "Epoch 141/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 183322.1250 - val_loss: 127303.2188\n",
      "Epoch 142/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 183672.1250 - val_loss: 127307.8594\n",
      "Epoch 143/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 182918.0312 - val_loss: 127195.7031\n",
      "Epoch 144/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 182940.3438 - val_loss: 127415.2734\n",
      "Epoch 145/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 182885.4219 - val_loss: 127384.7969\n",
      "Epoch 146/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 182825.4375 - val_loss: 126719.2031\n",
      "Epoch 147/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 183643.0000 - val_loss: 128534.1328\n",
      "Epoch 148/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 183219.5469 - val_loss: 126994.1719\n",
      "Epoch 149/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 183324.2656 - val_loss: 127934.4297\n",
      "Epoch 150/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 183146.7188 - val_loss: 127074.9531\n",
      "Epoch 151/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 183077.1719 - val_loss: 128325.0625\n",
      "Epoch 152/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 183194.4219 - val_loss: 126881.8594\n",
      "Epoch 153/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 182762.0781 - val_loss: 126804.6016\n",
      "Epoch 154/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 182939.9531 - val_loss: 127643.0469\n",
      "Epoch 155/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 183209.8281 - val_loss: 127101.5938\n",
      "Epoch 156/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 182997.0312 - val_loss: 126903.4531\n",
      "Epoch 157/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 183121.3125 - val_loss: 126618.0781\n",
      "Epoch 158/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 183043.6562 - val_loss: 126776.3125\n",
      "Epoch 159/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 183126.1094 - val_loss: 126827.5000\n",
      "Epoch 160/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 182944.8750 - val_loss: 127206.5781\n",
      "Epoch 161/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 182988.8438 - val_loss: 126847.2031\n",
      "Epoch 162/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 183417.2344 - val_loss: 127001.9453\n",
      "Epoch 163/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 183158.6562 - val_loss: 126763.1797\n",
      "Epoch 164/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 183095.9219 - val_loss: 127241.1484\n",
      "Epoch 165/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 183168.2344 - val_loss: 127136.3672\n",
      "Epoch 166/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 182973.2500 - val_loss: 126594.8984\n",
      "Epoch 167/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 182919.4688 - val_loss: 128298.3047\n",
      "Epoch 168/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 183833.9531 - val_loss: 127800.2422\n",
      "Epoch 169/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 182845.8906 - val_loss: 127274.3203\n",
      "Epoch 170/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 183539.4219 - val_loss: 126688.6016\n",
      "Epoch 171/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 182942.4844 - val_loss: 127713.7578\n",
      "Epoch 172/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 182784.2500 - val_loss: 126522.4297\n",
      "Epoch 173/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 182970.6562 - val_loss: 126319.6406\n",
      "Epoch 174/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 183184.5000 - val_loss: 126993.7344\n",
      "Epoch 175/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 182950.2031 - val_loss: 127214.4766\n",
      "Epoch 176/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 182970.3594 - val_loss: 126459.6406\n",
      "Epoch 177/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 183340.7031 - val_loss: 126216.8203\n",
      "Epoch 178/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 183198.3438 - val_loss: 127109.8203\n",
      "Epoch 179/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 182874.4375 - val_loss: 126560.9531\n",
      "Epoch 180/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 194372.5469 - val_loss: 127595.7031\n",
      "Epoch 181/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 182574.7969 - val_loss: 126442.6562\n",
      "Epoch 182/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 182922.9062 - val_loss: 126350.4844\n",
      "Epoch 183/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 183320.8750 - val_loss: 126715.1562\n",
      "Epoch 184/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 182815.7344 - val_loss: 126176.7500\n",
      "Epoch 185/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 182987.7188 - val_loss: 126572.1094\n",
      "Epoch 186/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 183186.5781 - val_loss: 126176.0859\n",
      "Epoch 187/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 182885.7031 - val_loss: 126682.1016\n",
      "Epoch 188/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 182751.7812 - val_loss: 126595.2891\n",
      "Epoch 189/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 183019.4531 - val_loss: 126676.4297\n",
      "Epoch 190/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 183155.1094 - val_loss: 130505.4141\n",
      "Epoch 191/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 184064.0781 - val_loss: 128377.3828\n",
      "Epoch 192/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 182785.3438 - val_loss: 126474.6094\n",
      "Epoch 193/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 182969.0156 - val_loss: 126492.9688\n",
      "Epoch 194/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 183858.7969 - val_loss: 126772.1875\n",
      "Epoch 195/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 183245.7031 - val_loss: 126445.0938\n",
      "Epoch 196/3000\n",
      "214/214 [==============================] - 2s 10ms/step - loss: 183225.2656 - val_loss: 126718.3594\n",
      "Epoch 197/3000\n",
      "209/214 [============================>.] - ETA: 0s - loss: 182810.3594"
     ]
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "n_outputs = X_train.shape[2]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(64, input_shape=(None, 4))))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "# model.add(Dense(n_outputs)) \n",
    "model.add(Dense(n_outputs))\n",
    "\n",
    "# model = keras.models.load_model('modelV2.h5')\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X_train, y_train, epochs=3000, batch_size=3000, verbose=1, validation_split=0.2)\n",
    "model.save('modelV2.h5')\n",
    "#loss: 847995.6250 - val_loss: 333785.3438"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab59824",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test, batch_size=3000)\n",
    "mse = ((y_pred - y_test)**2).mean(axis=0)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778f4e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Pred Pos: [{:f},{:f}] Spd: [{:f},{:f}]'.format(y_pred[0][0],y_pred[0][1],y_pred[0][2],y_pred[0][3]))\n",
    "print('Test Pos: [{:f},{:f}] Spd: [{:f},{:f}]'.format(y_test[0][0],y_test[0][1],y_test[0][2],y_test[0][3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4b2708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model('modelV2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaa7ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = y_pred\n",
    "b = y_test\n",
    "res = \"\\n\".join(\"{} {}\".format(x, y) for x, y in zip(a, b))\n",
    "print('MSE')\n",
    "print(mse)\n",
    "print('Pred                                                                  Real')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c84cc6",
   "metadata": {},
   "source": [
    "# Other Transformer\n",
    "\n",
    "https://keras.io/examples/nlp/text_classification_with_transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4d6c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e21cdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalMaxPooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.layers import TransformerBlock\n",
    "\n",
    "# Prepare the data\n",
    "input_shape = (10, 1)  # input shape of each time series sequence\n",
    "num_classes = 2  # number of classes for classification\n",
    "\n",
    "x_train = ...  # input time series data of shape (num_samples, input_shape)\n",
    "y_train = ...  # labels for the input data, shape (num_samples,)\n",
    "\n",
    "# Define the model\n",
    "inputs = Input(shape=input_shape)\n",
    "x = inputs\n",
    "\n",
    "embedding_dim = 128\n",
    "num_heads = 8\n",
    "ff_dim = 128\n",
    "\n",
    "# Time series embedding layer\n",
    "x = TransformerBlock(embed_dim=embedding_dim, num_heads=num_heads, ff_dim=ff_dim)(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=1e-4), metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "y_train = tf.keras.utils.to_categorical(y_train)  # convert labels to one-hot encoded vectors\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d0b198",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "https://keras.io/examples/timeseries/timeseries_transformer_classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a246702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def readucr(filename):\n",
    "    data = np.loadtxt(filename, delimiter=\"\\t\")\n",
    "    y = data[:, 0]\n",
    "    x = data[:, 1:]\n",
    "    return x, y.astype(int)\n",
    "\n",
    "\n",
    "root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\"\n",
    "\n",
    "x_train, y_train = readucr(root_url + \"FordA_TRAIN.tsv\")\n",
    "x_test, y_test = readucr(root_url + \"FordA_TEST.tsv\")\n",
    "\n",
    "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
    "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "idx = np.random.permutation(len(x_train))\n",
    "x_train = x_train[idx]\n",
    "y_train = y_train[idx]\n",
    "\n",
    "y_train[y_train == -1] = 0\n",
    "y_test[y_test == -1] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c7ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970f3007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n",
    "\n",
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8f99e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bc9445",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "model = build_model(\n",
    "    input_shape,\n",
    "    head_size=256,\n",
    "    num_heads=4,\n",
    "    ff_dim=4,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_units=[128],\n",
    "    mlp_dropout=0.4,\n",
    "    dropout=0.25,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=200,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "model.evaluate(x_test, y_test, verbose=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
